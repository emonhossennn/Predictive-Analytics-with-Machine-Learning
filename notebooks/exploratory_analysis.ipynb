{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analytics - Exploratory Data Analysis\n",
    "\n",
    "This notebook provides a template for exploratory data analysis and predictive modeling using the project's custom modules.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Data Loading and Overview](#data-loading)\n",
    "3. [Data Preprocessing](#preprocessing)\n",
    "4. [Exploratory Data Analysis](#eda)\n",
    "5. [Feature Engineering](#feature-engineering)\n",
    "6. [Model Training](#model-training)\n",
    "7. [Model Evaluation](#model-evaluation)\n",
    "8. [Predictions](#predictions)\n",
    "9. [Results and Conclusions](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports <a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Project modules\n",
    "from data_preprocessing import DataPreprocessor\n",
    "from model_training import ModelTrainer\n",
    "from model_evaluation import ModelEvaluator\n",
    "from prediction import Predictor\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Overview <a id=\"data-loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "# Replace 'your_dataset.csv' with your actual data file\n",
    "data_path = '../data/raw/your_dataset.csv'\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Uncomment when you have data\n",
    "# df = preprocessor.load_data(data_path)\n",
    "# print(f\"Dataset shape: {df.shape}\")\n",
    "# df.head()\n",
    "\n",
    "# For demonstration, let's create a sample dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Create synthetic data for demonstration\n",
    "df = pd.DataFrame({\n",
    "    'feature_1': np.random.normal(50, 15, n_samples),\n",
    "    'feature_2': np.random.normal(100, 25, n_samples),\n",
    "    'feature_3': np.random.exponential(2, n_samples),\n",
    "    'feature_4': np.random.uniform(0, 10, n_samples),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], n_samples),\n",
    "})\n",
    "\n",
    "# Create target variable with some relationship to features\n",
    "df['target'] = (0.5 * df['feature_1'] + \n",
    "                0.3 * df['feature_2'] + \n",
    "                0.2 * df['feature_3'] + \n",
    "                np.random.normal(0, 10, n_samples))\n",
    "\n",
    "print(f\"Sample dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "info = preprocessor.basic_info(df)\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Shape: {info['shape']}\")\n",
    "print(f\"Columns: {info['columns']}\")\n",
    "print(f\"Missing values: {info['missing_values']}\")\n",
    "print(f\"Duplicates: {info['duplicates']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing <a id=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values if any\n",
    "df_processed = preprocessor.handle_missing_values(df)\n",
    "\n",
    "# Encode categorical features\n",
    "df_processed = preprocessor.encode_categorical_features(df_processed)\n",
    "\n",
    "print(\"Data preprocessing completed!\")\n",
    "print(f\"Processed data shape: {df_processed.shape}\")\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis <a id=\"eda\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['target'], bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['target'])\n",
    "plt.title('Box Plot of Target Variable')\n",
    "plt.ylabel('Target')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot of features vs target\n",
    "feature_cols = [col for col in numeric_cols if col != 'target']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(feature_cols[:4]):\n",
    "    axes[i].scatter(df[feature], df['target'], alpha=0.6)\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Target')\n",
    "    axes[i].set_title(f'{feature} vs Target')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(df[feature], df['target'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[i].plot(df[feature], p(df[feature]), \"r--\", alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering <a id=\"feature-engineering\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features\n",
    "df_engineered = preprocessor.create_features(df_processed)\n",
    "\n",
    "print(f\"Original features: {df_processed.shape[1]}\")\n",
    "print(f\"After feature engineering: {df_engineered.shape[1]}\")\n",
    "print(f\"New features created: {df_engineered.shape[1] - df_processed.shape[1]}\")\n",
    "\n",
    "# Display new feature names\n",
    "new_features = [col for col in df_engineered.columns if col not in df_processed.columns]\n",
    "print(f\"\\nNew features: {new_features[:10]}...\")  # Show first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training <a id=\"model-training\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = preprocessor.split_data(df_engineered, 'target', test_size=0.2)\n",
    "\n",
    "# Scale features\n",
    "X_train_scaled = preprocessor.scale_features(X_train, fit=True)\n",
    "X_test_scaled = preprocessor.scale_features(X_test, fit=False)\n",
    "\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model trainer\n",
    "trainer = ModelTrainer(random_state=42)\n",
    "\n",
    "# Train multiple models\n",
    "trained_models = trainer.train_multiple_models(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Trained {len(trained_models)} models:\")\n",
    "for model_name in trained_models.keys():\n",
    "    print(f\"- {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation <a id=\"model-evaluation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Evaluate all models\n",
    "comparison_df = evaluator.evaluate_multiple_models(trained_models, X_test_scaled, y_test)\n",
    "\n",
    "# Display results\n",
    "print(\"Model Performance Comparison:\")\n",
    "comparison_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model\n",
    "best_model_name, best_model, best_score = trainer.find_best_model(X_test_scaled, y_test, metric='r2')\n",
    "print(f\"Best model: {best_model_name} with RÂ² score: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "evaluator.plot_model_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of best model\n",
    "evaluator.plot_predictions_vs_actual(best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals analysis\n",
    "evaluator.plot_residuals(best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (if supported)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    evaluator.feature_importance_plot(best_model, X_train_scaled.columns.tolist(), best_model_name)\n",
    "else:\n",
    "    print(f\"{best_model_name} doesn't support feature importance visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Predictions <a id=\"predictions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize predictor with best model\n",
    "predictor = Predictor(model=best_model)\n",
    "\n",
    "# Make predictions\n",
    "predictions = predictor.predict(X_test_scaled)\n",
    "\n",
    "print(f\"Made predictions for {len(predictions)} samples\")\n",
    "print(f\"Sample predictions: {predictions[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single prediction example\n",
    "sample_features = X_test_scaled.iloc[0].tolist()\n",
    "single_prediction = predictor.predict_single(sample_features)\n",
    "\n",
    "print(f\"Single prediction: {single_prediction:.2f}\")\n",
    "print(f\"Actual value: {y_test.iloc[0]:.2f}\")\n",
    "print(f\"Difference: {abs(single_prediction - y_test.iloc[0]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions with confidence intervals (for ensemble models)\n",
    "if hasattr(best_model, 'estimators_'):\n",
    "    pred, lower, upper = predictor.predict_with_confidence(X_test_scaled.iloc[:10])\n",
    "    \n",
    "    # Visualize confidence intervals\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x_range = range(len(pred))\n",
    "    \n",
    "    plt.plot(x_range, y_test.iloc[:10].values, 'o-', label='Actual', markersize=8)\n",
    "    plt.plot(x_range, pred, 's-', label='Predicted', markersize=8)\n",
    "    plt.fill_between(x_range, lower, upper, alpha=0.3, label='95% Confidence Interval')\n",
    "    \n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Target Value')\n",
    "    plt.title('Predictions with Confidence Intervals')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"{best_model_name} doesn't support confidence intervals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results and Conclusions <a id=\"results\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evaluation report\n",
    "report = evaluator.generate_evaluation_report()\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "model_save_path = '../models/best_model.pkl'\n",
    "trainer.save_model(best_model, model_save_path)\n",
    "print(f\"Best model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings:\n",
    "\n",
    "1. **Best Performing Model**: [To be filled based on results]\n",
    "2. **Performance Metrics**: [To be filled based on results]\n",
    "3. **Important Features**: [To be filled based on results]\n",
    "4. **Model Insights**: [To be filled based on results]\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Hyperparameter Tuning**: Fine-tune the best model\n",
    "2. **Feature Selection**: Remove irrelevant features\n",
    "3. **Cross-Validation**: Perform more robust validation\n",
    "4. **Deployment**: Prepare model for production\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "- [Add specific recommendations based on your analysis]\n",
    "- [Include business insights if applicable]\n",
    "- [Suggest improvements for model performance]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
